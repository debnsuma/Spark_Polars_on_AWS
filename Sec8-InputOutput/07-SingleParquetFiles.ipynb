{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8093b6a5-2db6-4cde-8cc5-caf21f5cfa6c",
   "metadata": {},
   "source": [
    "# Parquet files 1: Single Parquet files\n",
    "By the end of this lecture you will be able to:\n",
    "- read from a Parquet file\n",
    "- use query optimisation to read a subset of columns\n",
    "- get the schema of a Parquet file\n",
    "- write a Parquet file with compression\n",
    "\n",
    "\n",
    "## What is a Parquet file?\n",
    "A Parquet file is:\n",
    "- a *binary* file where data is ordered in columns rather than rows\n",
    "- each column has a name and a dtype\n",
    "- each column can be compressed separately with automatic dictionary encoding\n",
    "\n",
    "The Apache Parquet and Apache Arrow projects evolved together as columnar formats where Apache Parquet is the format for the data on disk and Apache Arrow is the format for the data in memory.\n",
    "\n",
    "Compared to CSV a Parquet file:\n",
    "- is faster to read and write than a CSV file\n",
    "- takes less space on disk, especially once compression is applied\n",
    "- allows Polars to select which columns to read without parsing the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619f72df-7ddd-42d0-b330-9e077cd5b4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5dc3985-7fe2-4f68-9ba1-3e1274f9cc0f",
   "metadata": {},
   "source": [
    "## Creating a Titanic Parquet file\n",
    "We begin by creating a Parquet file from the Titanic CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc3cf4c-6039-43de-af64-97c5a07c0fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"../data/titanic.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bd417d-a431-49d1-9618-3acc8bbd4b90",
   "metadata": {},
   "source": [
    "We create the Parquet Titanic directory in the `data_files/parquet` sub-directory of the `io` sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dc05e05-4b4a-4924-a1f5-a9f3b4952d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFilePath = Path(\"data_files/parquet/titanic\")\n",
    "if not parquetFilePath.exists():\n",
    "    parquetFilePath.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ca690d-4c94-4d2f-9370-8b4b4a6da113",
   "metadata": {},
   "source": [
    "Now we set the path that we will write the Parquet file to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69aed7b-7a31-4bae-8664-9862ac8f0716",
   "metadata": {},
   "outputs": [],
   "source": [
    "parquetFile = \"data_files/parquet/titanic/titanic.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940d2dfc-b56c-4f2c-949c-4838fe2d1b1e",
   "metadata": {},
   "source": [
    "We read the CSV and write to the Parquet path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f58883-e657-42db-af01-c1292c215554",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_csv(csvFile).write_parquet(parquetFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23cb1ae-2a83-40e0-be75-aae4950b4280",
   "metadata": {},
   "source": [
    "## Reading a Parquet file\n",
    "We read the Parquet file to a `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be78a2bd-2469-44b6-b77a-42b2944e4b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_parquet(parquetFile)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a6f8e01-fcdd-407d-b662-0aeb6246ca0a",
   "metadata": {},
   "source": [
    "As a Parquet file stores the schema as metadata we can get the schema of a Parquet file without having to read any data.\n",
    "\n",
    "In Polars we can use the `read_parquet_schema` function for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287cce1a-3e53-40bb-8cde-b2cbaf98d4ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.read_parquet_schema(parquetFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ab14c-418e-42e9-81f0-e6c7cfa52d10",
   "metadata": {},
   "source": [
    "We see that the dtypes are preserved in a Parquet file (unlike a CSV file where all data is converted to text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e720d4-5c90-4120-a7cf-f67ec2423bc5",
   "metadata": {},
   "source": [
    "We can select a subset of columns to read from a Parquet file with the `columns` argument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cc75dc2-8d42-4d78-bb1f-4f35e54ecff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquetFile,\n",
    "        columns=[\"Pclass\",\"Name\"]\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fcb744-364c-4884-8371-92f9142b5cbe",
   "metadata": {},
   "source": [
    "When we work in lazy mode in Polars the query optimiser will select a subset of columns automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22cd84b5-c108-4849-8558-078a0da431e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_parquet(parquetFile)\n",
    "    .select([\"Pclass\",\"Name\"])\n",
    "    .describe_optimized_plan()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510eefdb-01d2-4b9d-9400-0bc7924084b8",
   "metadata": {},
   "source": [
    "We can also specify a smaller number of rows that we want to read with `n_rows`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883628bb-19a0-49de-9423-9bec787051ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquetFile,\n",
    "        n_rows=2\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634d0cbb-8ca4-4160-b51e-ccf582e8db1d",
   "metadata": {},
   "source": [
    "If we are running out of memory when reading a Parquet file we can specify `low_memory = True`. This can help to reduce peak memory usage at the expense of a longer load time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8776668-910f-4fbd-a0c9-82ffcd80d54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_parquet(\n",
    "        parquetFile,\n",
    "        low_memory=True\n",
    "    )\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f8b4f4-5305-4abc-9ddb-88ae04f030d3",
   "metadata": {},
   "source": [
    "Polars reads the Parquet file in multiple threads into different chunks of memory. By default Polars then combines all the chunks into a single chunk in parallel. With the `low_memory=True` argument Polars reduces peak memory usage by not doing this recombination in parallel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39803e28-01e7-401f-a750-f9fdc423eba2",
   "metadata": {},
   "source": [
    "Using `low_memory = True` will not help if the ultimate `DataFrame` does not fit in memory. In this case using `streaming` in lazy mode is the best option"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcee5c16-2b32-488a-b533-cc94cc8a6e33",
   "metadata": {},
   "source": [
    "## Writing a Parquet file\n",
    "When we write a Parquet file we can specify compression options. I recommend using `zstd` in most cases for a good balance of compressed file size on disk and read time into memory. The `lz4` option is an alternative when faster reading and writing is preferred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1adadc-5e0e-4568-8b67-076729087844",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write_parquet(parquetFile,compression=\"zstd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3722e140-d642-4c59-8cd2-70054b2d2d9a",
   "metadata": {},
   "source": [
    "We can also adjust the degree of compression with `compression_level`. The range of values depends on the compression scheme chosen - see the docstrings for details.\n",
    "\n",
    "## Exercises\n",
    "In the exercises you will develop your understanding of:\n",
    "- read and writing Parquet files\n",
    "- categorical dtypes in Parquet files\n",
    "- reading the schema of Parquet files\n",
    "- reading a subset of Parquet files\n",
    "\n",
    "### Exercise one\n",
    "We will write a new Parquet file for the exercises to this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f894f-3c42-4484-97b6-0c35fc2ace0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "exerciseParquetFile = \"data_files/parquet/titanic/titanic_exercise.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9cb840-19fd-4dad-97f4-d8256d5f789a",
   "metadata": {},
   "source": [
    "Before we write to this file read the Parquet file created at the start of the notebook to a `DataFrame`. \n",
    "\n",
    "Convert the `Sex` column to `pl.Categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea1b6dd-42fe-429a-b508-8100b6875e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_parquet(parquetFile)\n",
    "    .with_column(<blank>)\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2641107b-5c72-4b27-9994-dc6b1f4fe9ab",
   "metadata": {},
   "source": [
    "Write the `DataFrame` with a categorical column to `exerciseParquetFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb60f32d-bcd0-46cc-9a10-dca657df0b60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f29c4c7c-9ea1-4282-8387-4c3c6c1bfdb4",
   "metadata": {},
   "source": [
    "Read the schema of `exerciseParquetFile` to confirm whether Parquet can preserve categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb333b54-9d1f-4cdc-8780-e376ad4fb654",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7c209952-ea58-48b9-b5c9-82fddb489344",
   "metadata": {},
   "source": [
    "Create a lazy query that only reads these columns\n",
    "```python\n",
    "[\"Survived\",\"Pclass\",\"Age\",\"Sex\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62ab6ea-c2aa-43a8-b29c-be62b0fe2810",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    <blank>\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dfb0e3-4ce8-497a-a8ea-02d2051b92f2",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise one\n",
    "We will write a new Parquet file for the exercises to this path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2355ad-09ca-4463-b110-ee01c3ecccdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "exerciseParquetFile = \"data_files/parquet/titanic/titanic_exercise.parquet\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43db9a-ea2c-44d8-8808-952fa3237c98",
   "metadata": {},
   "source": [
    "Before we write to this file read the Parquet file created at the start of the notebook to a `DataFrame`. \n",
    "\n",
    "Convert the `Sex` column to `pl.Categorical`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403c8a61-e682-47f8-9cc8-0b1b29ee5177",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = (\n",
    "    pl.read_parquet(parquetFile)\n",
    "    .with_column(pl.col(\"Sex\").cast(pl.Categorical))\n",
    ")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113b9157-2688-4764-9d48-597175eec3d3",
   "metadata": {},
   "source": [
    "Write the `DataFrame` with a categorical column to `exerciseParquetFile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501bcf8-305c-4bd9-80ca-e4554b43d347",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.write_parquet(exerciseParquetFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd2f63c-f793-480c-8fc4-43f93588c848",
   "metadata": {},
   "source": [
    "Read the schema of `exerciseParquetFile` to confirm whether Parquet can preserve categorical encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1490306b-fd6f-4400-aac4-554b2fdafad8",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_parquet_schema(exerciseParquetFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a5df38-7deb-4e8b-ae26-33763f0ec9e9",
   "metadata": {},
   "source": [
    "Create a lazy query that only reads these columns\n",
    "```python\n",
    "[\"Survived\",\"Pclass\",\"Age\",\"Sex\"]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5298bffa-95e6-4b89-891a-dd81af45bbfe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_parquet(exerciseParquetFile)\n",
    "    .select([\"Survived\",\"Pclass\",\"Age\",\"Sex\"])\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
