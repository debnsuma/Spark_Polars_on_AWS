{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f24a78fa-fb7b-4f26-bd78-2883a39f0a89",
   "metadata": {},
   "source": [
    "# CSV files 2: multiple files\n",
    "By the end of this lecture you will be able to:\n",
    "- read multiple CSV files with a glob pattern\n",
    "- read multiple CSV files from a list\n",
    "- read multiple CSV files in lazy mode\n",
    "- automate CSV discovery in sub-directories\n",
    "\n",
    "We import Python's built-in `pathlib` module to work with multiple file paths and create sub-directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b939b3a1-683b-4b50-b684-3ab989daf676",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import polars as pl\n",
    "\n",
    "pl.Config.set_tbl_rows(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edc655e-ee7e-487b-80d4-fee28463ac3a",
   "metadata": {},
   "source": [
    "We need a dataset with multiple CSV files that share the same scheme for this notebook.\n",
    "\n",
    "We create multiple CSV files from the Titanic dataset in a new directory.\n",
    "\n",
    "We begin by reading in the full CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa380c6-464c-44e7-b627-fb373e0d30db",
   "metadata": {},
   "outputs": [],
   "source": [
    "csvFile = \"../data/titanic.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebcfbf4-4aea-47a2-af2f-725a274790ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pl.read_csv(csvFile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f47946-e2af-47eb-b2ed-99d6da53677e",
   "metadata": {},
   "source": [
    "We create a new sub-directory in this directory.\n",
    "\n",
    "We use the `mkdir` method of a `Path` object to create this new sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b023c963-e064-403f-affe-7717b4c70cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new directory\n",
    "csvDirectory = Path(\"data_files/csv/multiple_csv\")\n",
    "# Create the new directory if it doesn't already exist\n",
    "csvDirectory.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b1c0e3-1dc4-4018-b894-2d5563a5b98a",
   "metadata": {},
   "source": [
    "We split the `DataFrame` and write the two new files to the sub-directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d9173-0b40-4ec1-863c-ccb059144e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[:700].write_csv(csvDirectory / \"train.csv\")\n",
    "df[700:].write_csv(csvDirectory / \"test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa575804-c0ab-4630-b11e-7bc6f3c0b6cd",
   "metadata": {},
   "source": [
    "## Reading CSVs in eager mode\n",
    "### Reading multiple files with wildcard patterns\n",
    "\n",
    "We can read multiple CSV files with the same scheme using wildcard patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa006e3-b8e7-4791-a33f-6220d7129bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(csvDirectory / \"*.csv\")\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f52cb23-9f1f-4add-81d2-28397585ee60",
   "metadata": {},
   "source": [
    "The files are read in alphabetical order where `test` comes before `train`.\n",
    "\n",
    "If you prefer to work in raw file paths the cell above is equivalent to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f256ba-b5ef-4406-840a-6125ed12aa38",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(\"data_files/csv/multiple_csv/*.csv\")\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866d6ba4-5e06-4364-8416-7598dc50ac82",
   "metadata": {},
   "source": [
    "#### What happens when we use the wildcard pattern `*`?\n",
    "When we use the wildcard pattern `*` Polars calls `scan_csv` and does a lazy query to combine all the matching files - this is an automatic version of the lazy mode we see below!\n",
    "\n",
    "However, unlike the manual version we see below we do not have access to the query optimiser. This means that if we follow `read_csv` with - for example - a `filter` method each file is still be read in full and then the `filter` is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ffe449d-d94e-4390-9f29-0a52f592f5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(\"data_files/csv/multiple_csv/*.csv\")\n",
    "    .filter(pl.col(\"Pclass\") == 1)\n",
    "    .head(2)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1e592-82b5-4c88-97c3-e963c047ce62",
   "metadata": {},
   "source": [
    "### Reading from a list of file paths\n",
    "\n",
    "If we have a list of file paths we can also read them manually with `pl.concat`.\n",
    "\n",
    "#### Making a file path generator\n",
    "\n",
    "In this example we call `glob` on the `csvDirectory` `Path` object to make `filePathsGenerator`.\n",
    "\n",
    "The `filePathsGenerator` object is a Python `generator`. We can loop through a generator like a list and to produce the next element. If you are not familar with generators [check out the excellent Whirlwind Tour of Python](https://jakevdp.github.io/WhirlwindTourOfPython/12-generators.html) for an introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d10fcb-cca7-444d-93bd-45f28d8b8371",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePathsGenerator = csvDirectory.glob(\"*csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fff7f95-9e06-41b5-b9bf-2080f66f36d4",
   "metadata": {},
   "source": [
    "#### Iterating through the generator\n",
    "We iterate through the files from the generator. \n",
    "\n",
    "If we want to re-run this cell we need to re-run the cell above first to reset `filePathsGenerator` to the start of the file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e50f536-9fa6-4a94-bd08-acefeb8ca1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.concat(\n",
    "        [pl.read_csv(csvPath) for csvPath in filePathsGenerator]\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838fb917-edd7-49d1-971c-0dc62de8cefa",
   "metadata": {},
   "source": [
    "## Scanning CSVs in lazy mode\n",
    "\n",
    "### Scanning multiple files with a wildcard\n",
    "We can scan multiple CSV files in a directory in lazy mode using a wildcard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7befb1c-231a-4efd-a9a4-48449986edfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    pl.scan_csv(csvDirectory / \"*.csv\")\n",
    "    .filter(pl.col(\"Age\") > 50)\n",
    "    .describe_optimized_plan()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c3cb226-6b8c-47db-9662-573b8a454bf9",
   "metadata": {},
   "source": [
    "The plan shows us that Polars:\n",
    "- carries out the normal `CSV SCAN` on each file including the `SELECTION` on `Age` to create a chunk in memory for each file\n",
    "- connects the chunks internally to a single object in `UNION`\n",
    "- does a `RECHUNK` to combine the chunks from each file into a single chunk for the `DataFrame`\n",
    "\n",
    "> The `RECHUNK` is optional. In some queries it may be faster to set `rechunk = False` in `pl.scan_csv`\n",
    "\n",
    "We evaluate this plan on all the CSVs with `collect`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe14efc-1493-43ba-b3e1-ad3038dba9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_csv(csvDirectory / \"*.csv\")\n",
    "    .filter(pl.col(\"Age\") > 50)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce3e50-26f2-4edf-9367-81c70c961b26",
   "metadata": {},
   "source": [
    "### Scanning from a list of file paths in lazy mode\n",
    "We can also create a list of scanned CSV files in lazy mode.\n",
    "\n",
    "We re-define the `filePathsGenerator` in this cell so that we can iterate through it again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80613e6f-5d54-4184-acf4-b8732fa3ec31",
   "metadata": {},
   "outputs": [],
   "source": [
    "filePathsGenerator = csvDirectory.glob(\"*csv\")\n",
    "queriesList = [\n",
    "    pl.scan_csv(csvPath) for csvPath in filePathsGenerator\n",
    "]\n",
    "queriesList"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f51e070-924a-485f-84cf-754a2e22de0c",
   "metadata": {},
   "source": [
    "The `queriesList` is a `list` of `LazyFrames`.\n",
    "\n",
    "Polars can evaluate a `list` of `LazyFrames` with `pl.collect_all`.  The output is a `list` of `DataFrames`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2725870-8d05-4c32-832c-f3528440b02f",
   "metadata": {},
   "source": [
    "To return the output as a single `DataFrame` we call:\n",
    "- `pl.collect_all` on the `list` to return a `list` of `DataFrames`\n",
    "- `pl.concat` the combine the `list` of `DataFrames` to a single `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ad2956-fac8-4109-baf0-d0e47a7ca7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.concat(\n",
    "    pl.collect_all(queriesList)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2164a46-2474-42a4-979d-ac83547c9fd5",
   "metadata": {},
   "source": [
    "When you call `pl.collect_all` Polars runs `pl.collect` on each element in parallel.\n",
    "\n",
    "For large datasets we can use streaming with `streaming = True` in `pl.collect_all`. In this case Polars reads any large CSVs in batches.\n",
    "\n",
    "## Discovering file paths\n",
    "In some cases we want an easy way to find all the CSVs in sub-directories.\n",
    "\n",
    "We can use PyArrow in this case. While using PyArrow isn't necessary in this simple example, it is handy with more complicated directory structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18ac785-feb7-4f60-b852-faeddb6a09a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow.dataset as ds\n",
    "\n",
    "dataset = ds.dataset(\n",
    "    csvDirectory,\n",
    "    format=\"csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0674e9-f801-4d33-8e24-7c006d9042b4",
   "metadata": {},
   "source": [
    "We list the files that PyArrow has found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80efdd0f-b9e4-461c-933d-ae46630763f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcbd0da5-5dc6-459b-89ee-05367e751f98",
   "metadata": {},
   "source": [
    "We can then read these files in eager mode by:\n",
    "- letting PyArrow turn them into an Arrow table and\n",
    "- creating a Polars `DataFrame` from the Arrow table with zero-copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2829732c-1c61-49df-a1fd-4ca94caeef7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.from_arrow(\n",
    "        dataset.to_table()\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e01a0e0-8ddd-47e2-bf88-11c255582c16",
   "metadata": {},
   "source": [
    "With PyArrow we can do manual optimisations such as limit the columns or apply a row filter in the arguments of `to_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3e9e47-869d-4482-a521-294fe62e0a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.from_arrow(\n",
    "        dataset.to_table(\n",
    "            columns=[\"Pclass\",\"Age\"],\n",
    "            filter = ds.field(\"Age\") > 70)\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6aa378-5356-4925-885e-854375e1255a",
   "metadata": {},
   "source": [
    "See the PyArrow docs for more info on the `dataset` object: https://arrow.apache.org/docs/python/dataset.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2518604-4790-4eb2-8172-3fd0d6a288cc",
   "metadata": {},
   "source": [
    "## So which approach should you use?\n",
    "Each of these approaches will work, but these are my opinions for general cases:\n",
    "- If you want to read all files into memory with no query optimisations use `pl.read_csv`\n",
    "- Use a wildcard if you can specify the files using a wildcard\n",
    "- Use a list if you want more control over which files you read\n",
    "- Use PyArrow if you have a more complicated directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baf36629-d239-476a-96a4-ab64bb64fe61",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "In the exercises you will develop your understanding of:\n",
    "- reading multiple CSV files in eager mode\n",
    "- reading multiple CSV files in lazy mode\n",
    "- reading CSVs with PyArrow\n",
    "\n",
    "### Exercise 1\n",
    "The NYC taxi dataset CSV has 1000 rows containing records from different days.\n",
    "\n",
    "### Set-up\n",
    "We transform this CSV into a set of partitioned CSVs in sub-directories. \n",
    "\n",
    "We first set the path to the full CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2adb08d2-e9bd-459c-a069-abb138d03670",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycCsvFile = \"../data/nyc_trip_data_1k.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57da09fe-bdb1-464d-941a-733ef7a987ed",
   "metadata": {},
   "source": [
    "We now:\n",
    "- read the CSV\n",
    "- add a column that records the date from the `pickup` datetime\n",
    "- partition the `DataFrame` into a dictionary that maps dates to the `DataFrame` for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af22981b-1e28-4110-af6c-0c54076edb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDfDict = (\n",
    "    pl.read_csv(nycCsvFile,parse_dates=True)\n",
    "    .with_column(\n",
    "    pl.col(\"pickup\").dt.truncate(\"1d\").dt.strftime(\"%Y-%m-%d\").alias(\"pickup_day\")\n",
    "    )\n",
    "    .partition_by(\"pickup_day\",as_dict=True)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb985d-08da-439d-bfcb-f1e02b81d2c8",
   "metadata": {},
   "source": [
    "The keys of the `dailyDfDict` are the string dates for each day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1049dff-5323-49a2-97d8-50051ab78cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDfDict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84340d4-c23a-4a3d-ac57-0cc49297b0f4",
   "metadata": {},
   "source": [
    "The values for each key is a `DataFrame` for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32757ce9-a002-42d4-a6cb-96f6bf20ce57",
   "metadata": {},
   "outputs": [],
   "source": [
    "dailyDfDict['2022-01-01'].head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e1e13e-55ae-4401-9eff-a00899abad9b",
   "metadata": {},
   "source": [
    "We now create a partitioned directory called `daily_nyc` for the data.\n",
    "\n",
    "The name of each sub-directory is a date.\n",
    "\n",
    "The content of each sub-directory is the CSV for that date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06b3134-e608-451e-9506-27f8209f39c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the new directory\n",
    "nycCsvDirectory = Path(\"data_files/csv/daily_nyc\")\n",
    "\n",
    "# Create the new directory if it doesn't already exist\n",
    "nycCsvDirectory.mkdir(parents=True,exist_ok=True)\n",
    "\n",
    "# Loop through each date\n",
    "for day, df in dailyDfDict.items():\n",
    "    # Create a Path object for that date\n",
    "    dailyDirectory = (nycCsvDirectory / day)\n",
    "    # Create the sub-directory for that date\n",
    "    dailyDirectory.mkdir(parents=True,exist_ok=True)\n",
    "    # Write a CSV called daily.csv\n",
    "    df.write_csv(dailyDirectory / \"daily.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a27b57f-d0aa-466a-9008-7446a138a196",
   "metadata": {},
   "source": [
    "We list the contents of `daily_nyc` to see the sub-directories for each date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37428579-5150-4c69-a7dc-046d23817058",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data_files/csv/daily_nyc/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6feadc7-c6d4-4031-82cd-4675ef6ffe2d",
   "metadata": {},
   "source": [
    "We list the contents of one sub-directory to show the CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2841ef-7f3d-4531-a03c-72f26514f360",
   "metadata": {},
   "outputs": [],
   "source": [
    "ls data_files/csv/daily_nyc/2022-01-01/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c2dcb7-acf0-449c-8a96-b859357a3834",
   "metadata": {},
   "source": [
    "### Now on to the exercise!\n",
    "\n",
    "Read all the CSV files in eager mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78f8b7f-4974-4520-a1cb-21c0ba5ebfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    pl.read_csv(\n",
    "        \"data_files/csv/daily_nyc<blank>\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e2df8f-4ed7-4b14-991e-8b8807ce3867",
   "metadata": {},
   "source": [
    "Read the CSV files in eager mode using:\n",
    "- a `glob` and a `generator`\n",
    "- a concatenation of the list of `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68962c4d-9b9f-4c9f-a4ff-93fcd25edf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "nycFilePathsGenerator = nycCsvDirectory<blank>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ba8a1e-5706-4b0d-91cb-e176c5d99127",
   "metadata": {},
   "source": [
    "Read all the CSV files in lazy mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdaff2f-3222-489e-b4c8-5c0e9a18459f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "30725747-3765-41d4-a69f-7982ef4ba9d3",
   "metadata": {},
   "source": [
    "Read all the CSVs in lazy mode **between 2022-01-01 and 2022-01-09** inclusive\n",
    "\n",
    "- Scan the required `DataFrames` by iterating through the generator\n",
    "- Call `collect_all` to evaluate all the `LazyFrames`\n",
    "- `concat` all the `DataFrames`\n",
    "\n",
    "If you want a hint about filtering the dates expand the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e507db69-c36e-463e-a89d-342813ad79a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Hint: in an `if` statement convert the `csvPath` to string with `csvPath.as_posix()` and check if 2022-01-0\n",
    "# is in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "018551e6-b09e-41cd-b9d6-cb44b886f296",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ce7b7557-abda-48c2-b37d-0a426dfe3d6b",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Create a PyArrow `dataset` object with all the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30751d20-e153-4756-8b12-a683ba8035fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b112bc7e-3da6-4f33-8882-77b845a1fa2f",
   "metadata": {},
   "source": [
    "List all the CSV files in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373439fe-82d6-447c-b3c9-aed93dfa4ca3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1c9097d7-c087-4504-ab84-4df184f5457d",
   "metadata": {},
   "source": [
    "Read all the files into a Polars `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2c8b51-5cc7-47f6-bb18-15815cb4a9ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f65a5073-474c-4f7c-9af7-1af067b90e71",
   "metadata": {},
   "source": [
    "## Solutions\n",
    "\n",
    "### Solution to exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca15b09-1b6f-41ad-8f89-1bdb7fcf2d6f",
   "metadata": {},
   "source": [
    "Read all the CSV files in eager mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5606bc16-7137-463a-a6bc-17bbc3d415da",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pl.read_csv(\"data_files/csv/daily_nyc/*/daily.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8faa5876-af30-4e29-a91a-5418715819fa",
   "metadata": {},
   "source": [
    "Read the CSV files in eager mode using:\n",
    "- a `glob` and a `generator`\n",
    "- a concatenation of the list of `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e66739a-eaa5-4932-af48-1afb80160e1b",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "filePathsGenerator = nycCsvDirectory.glob(\"*/*.csv\")\n",
    "(\n",
    "    pl.concat(\n",
    "        [pl.read_csv(csvPath) for csvPath in filePathsGenerator]\n",
    "    )\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1119bf9-51aa-445d-b5c0-91b9bf911fa5",
   "metadata": {},
   "source": [
    "Read all the CSV files in lazy mode using a path with wildcards for the final directory name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7647b52-9047-4eea-89be-f143800157dd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.scan_csv(\"data_files/csv/daily_nyc/*/daily.csv\")\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60392580-06e2-4a7e-95d6-4e446c528a96",
   "metadata": {},
   "source": [
    "Read all the CSVs in lazy mode *between 2022-01-01 and 2022-01-09** inclusive\n",
    "\n",
    "- Scan the required `DataFrames` by iterating through the generator\n",
    "- Call `collect_all` to evaluate all the `LazyFrames`\n",
    "- `concat` all the `DataFrames`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b08dfe5-83a3-4197-8c2d-1c222502dbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hint: in an `if` statement convert the `csvPath` to string with `csvPath.as_posix()` and check if 2022-01-0\n",
    "# is in the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1165117c-de0f-4224-ba09-f5bec0f81ee6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nycFilePathsGenerator = nycCsvDirectory.glob(\"*/daily.csv\")\n",
    "(\n",
    "    pl.concat(\n",
    "        pl.collect_all(\n",
    "            [pl.scan_csv(csvPath) for csvPath in nycFilePathsGenerator if \"2022-01-0\" in csvPath.as_posix()]\n",
    "        )\n",
    "    )\n",
    ").shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00b9673-f0fb-4c62-9d32-e3db404cceeb",
   "metadata": {},
   "source": [
    "### Solution to exercise 2\n",
    "Create a PyArrow `dataset` object with all the CSVs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6606fda8-3ecc-4129-85e4-5a1156d875a5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset = ds.dataset(nycCsvDirectory,format=\"csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3eca454-eac9-44d1-854d-ad724d2132c8",
   "metadata": {},
   "source": [
    "List all the CSV files in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0f47f7-971a-4ef6-9bb0-598688e3181e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dataset.files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3301b5db-eb8d-4f7f-a8ec-23e8116a0152",
   "metadata": {},
   "source": [
    "Read all the files into a Polars `DataFrame`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e193abab-2fff-4adf-8c3b-326ddc9f1de6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "(\n",
    "    pl.from_arrow(\n",
    "        dataset.to_table()\n",
    "    )\n",
    "    .head(3)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
